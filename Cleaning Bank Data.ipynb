{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I used a mix of pyspark and pandas throughout this notebook, sometimes alternating\n",
    "# to show the same command carried out both in pyspark and pandas, between different questions.\n",
    "# I did this because I assumed that at some point, big data processes might become\n",
    "# important for this analyst team at N26."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1\n",
    "# Identify all customers that we need to report as tax-liable \n",
    "# based on their personal data and the following criteria. \n",
    "# Only ONE of the following criteria needs to be met \n",
    "# for a customer to be included in the reported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All customers with tax IDs in Germany, France, Italy or Spain.\n",
    "### tax_id file. 'country' variable.\n",
    "\n",
    "# All customers that live in Germany, France, Italy or Spain.\n",
    "### customer_data file. 'country' variable\n",
    "\n",
    "# All customers that have phone numbers of Germany, France, Italy or Spain.\n",
    "### customer_data file. 'mobile_phone_no' variable (respective area codes: 49, 33, 39, 34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because customers only have to fit one of these critera, I broke the selection process\n",
    "# into 3 steps, that I then merged together again at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "import pyspark\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "import pandas as pd\n",
    "\n",
    "conf = pyspark.SparkConf().setAppName('appName').setMaster('local')\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the customer data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer = (spark.read.format(\"csv\").options(header=\"true\")\n",
    "    .load(\"customer_data.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tax data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxes = (spark.read.format(\"csv\").options(header=\"true\")\n",
    "    .load(\"tax_ids.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the user_id naming consistent across files.\n",
    "customer = customer.withColumnRenamed('id','user_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify all customers who live in the 4 countries, from original customer file.\n",
    "customer_country = customer[customer.country.isin(\"Germany\", \"France\",\"Italy\", \"Spain\")].collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_cust_country = spark.createDataFrame(customer_country)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_cust_country.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lit\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify all customers who have phone numbers from the 4 countries, from original customer file.\n",
    "phone = customer.filter(\"mobile_phone_no like '+49%' OR mobile_phone_no like '+33%' OR mobile_phone_no like '+39%' OR mobile_phone_no like '+34%'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the reduced data set to pandas. Going spark to pandas can be useful for very big data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_phone = phone.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_country = spark_cust_country.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify all customers who have tax IDs in the 4 countries.\n",
    "tax_country = taxes[taxes.country.isin(\"Germany\", \"France\",\"Italy\", \"Spain\")].collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_tax = spark.createDataFrame(tax_country)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_tax = spark_tax.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary\n",
    "# Create a for loop that goes through each row and makes the user_id the key\n",
    "# There can only be one key, so this effectively collects all the country and tax_id\n",
    "# information for each user_id. This way, we don't lose duplicates.\n",
    "taxdict = {}\n",
    "for x in range(len(pd_tax)):\n",
    "    currentid = pd_tax.iloc[x,0]\n",
    "    currentvalue = pd_tax.iloc[x,1]\n",
    "    currentvalue2 = pd_tax.iloc[x,2]\n",
    "    taxdict.setdefault(currentid, [])\n",
    "    taxdict[currentid].append({'country' : currentvalue,'id' : currentvalue2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn the dictionary into a list, with both key and values together\n",
    "list_key_value = [ [k,v] for k, v in taxdict.items() ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check a specific observation, previously identified as having 2 tax IDs.\n",
    "list_key_value[11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the tax_information variable shown in the .json file.\n",
    "back_to_pd_tax = pd.DataFrame(list_key_value, columns = ['user_id', 'tax_information'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the tax with the meta customer file, to fill in blanks for the tax customers.\n",
    "# Assumption: null values for tax_information (as opposed to empty lists) are acceptable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_customer = customer.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_merge = pd.merge(pd_customer, back_to_pd_tax, how='outer', on='user_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_merge.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_merge = meta_merge.dropna(subset=['tax_information'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_merge.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the customer data for phone and country.\n",
    "first_merge = pd.merge(pd_phone, pd_country, how='outer', on=['user_id', 'country','iban',\n",
    "            'account_opened', 'mobile_phone_no', 'account_closed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the customer data with the tax data\n",
    "question_one_final = pd.merge(first_merge, meta_merge, how='outer', on=['user_id', 'country','iban',\n",
    "            'account_opened', 'mobile_phone_no', 'account_closed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_one_final.to_csv(r\"D:\\School\\CVs\\Germany\\N26\\case_reporting_data_analyst\\question_one_final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2\n",
    "# Identify all customers that we need to report as tax-liable \n",
    "# based on their transaction data. \n",
    "# Any customer with at least one transaction that \n",
    "# fulfills ALL of the following criteria is tax-liable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transaction in 2018.\n",
    "### Transaction data file. 'date' variable.\n",
    "\n",
    "#Transaction type is a direct debit.\n",
    "### Transaction data file. 'type' variable.\n",
    "\n",
    "#Recipient IBAN is in Germany, France, Italy or Spain. = transactions\n",
    "### Transaction data file. 'counterparty' variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since customer have to fit all 3 criteria, I wittled down the file step by step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read file directly as pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_transaction = pd.read_csv('transactions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_transaction.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn the 'date' variable into a datetime variable.\n",
    "pd_transaction['date'] = pd.to_datetime(pd_transaction['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the start and end dates of the year, 2018.\n",
    "start_date = \"2018-01-01\"\n",
    "end_date = \"2018-12-31\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a mask to filter out the values that are not from 2018 (none)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (pd_transaction['date'] >= start_date) & (pd_transaction['date'] <= end_date )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting only dates from 2018\n",
    "pd_transaction = pd_transaction.loc[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_transaction.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new file that has only direct debit transactions. From working a lot with SAS, I tend to create extra\n",
    "# data frames, for backup/double checking.\n",
    "pd_deb_clear = pd_transaction[pd_transaction.type == 'direct_debit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using a pandas way, as opposed to the previously used pyspark way, to select\n",
    "# observations from the 4 countries.\n",
    "question_two_penul= pd_deb_clear[pd_deb_clear['counterparty'].str.startswith(('FR', 'IT', 'ES', 'DE'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copying only the user column because we don't need the other columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_cleaning = question_two_penul['user_id'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only selecting one column meant that the results became a series. Thus I convert it back to a df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_cleaning = two_cleaning.to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and add a column that indicates that all 3 criteria were met. This makes it easier for me to keep track\n",
    "# of these observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_cleaning['criteria_met']=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping any duplicates. This is safe because we only need to report customers once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_cleaning = two_cleaning.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because the transaction data didn't have any IBAN numbers, I load the customer file and merge it with\n",
    "# the reduced transaction file (i.e., criteria_met file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iban = pd.read_csv('customer_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iban = iban.rename(columns={\"id\": \"user_id\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge on inner, because we only want matching records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iban_fill = pd.merge(two_cleaning, iban, how='inner', on='user_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the end results of question/task 1 and 2. Outer merge because we want all records from both files.\n",
    "# Merging on many variables so as to not end up with duplicate columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_one_two = pd.merge(question_one_final, iban_fill, how='outer', on=['user_id', 'country','iban',\n",
    "            'account_opened', 'mobile_phone_no', 'account_closed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3\n",
    "# Report only customers that had an active account in 2018 \n",
    "#(had an open account on at least one day in 2018)\n",
    "### customer data file. 'account_opened' and 'account_closed' variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_customers = question_one_two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn the 'account_opened' and 'account_closed' variables into datetime variables.\n",
    "pd_customers['account_opened'] = pd.to_datetime(pd_customers['account_opened'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_customers['account_closed'] = pd.to_datetime(pd_customers['account_closed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set start and stop dates for opening and closing of accounts\n",
    "# Remove anyone who opened an account after the last day of 2018\n",
    "# Removed anyone who closed their account before the first day of 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opened = \"2018-12-31\"\n",
    "closed = \"2018-01-01\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_customers = pd_customers.drop(pd_customers[(pd_customers.account_opened > opened)].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_customers = pd_customers.drop(pd_customers[(pd_customers.account_closed < closed)].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = pd_customers.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a table of all users identified based on 1), 2) and 3) and ensure that we \n",
    "#do not double-report any customer. We need to report \n",
    "#a customerâ€™s IBAN, account opening, account closure dates and, \n",
    "#if available their tax country and IDs. \n",
    "#Be aware that we do not have tax IDs for all customers. \n",
    "#In that scenario, it suffices that we report IBAN, account opening and \n",
    "#closure dates as well as an empty tax information line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns that we don't want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report.drop([\"user_id\",\"country\",\"mobile_phone_no\", \"criteria_met\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns to match example file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = report.rename(columns={\"account_opened\": \"opened\", \"account_closed\": \"closed\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Date time objects need to be converted to string before we convert to json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also, the format had to be changed. This does both, for the 'opened' variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report['opened'] = report['opened'].apply(lambda x: x.strftime('%Y-%m-%d'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This does the same for the 'closed' variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report['closed'] = report['closed'].dt.strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing the \"NaT\" values with \"null\" values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report['closed'] = report['closed'].apply(lambda x : \"null\" if x==\"NaT\" else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the users as a JSON file with the layout as outlined in example_output.json."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report.to_json(r'D:\\Germany\\N26\\case_reporting_data_analyst\\report_final.json', orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_json_pretty = json.dumps(json.loads(report.to_json()), indent=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
